{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'test_V2.csv', 'train_V2.csv', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('bone')\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "print(os.listdir(\"../data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "1b7c01604a8239f1222e7a7dc9d8562af10881f0"
   },
   "outputs": [],
   "source": [
    "def toTapleList(list1,list2):\n",
    "    return list(itertools.product(list1,list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (1, 5), (1, 6), (2, 4), (2, 5), (2, 6), (3, 4), (3, 5), (3, 6)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toTapleList([1,2,3],[4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "3519f56ce1cefbb965fc84b4890b5093f0f3d05f"
   },
   "outputs": [],
   "source": [
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                #    df[col] = df[col].astype(np.float16)\n",
    "                #el\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        #else:\n",
    "            #df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n",
    "        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e220d486a8b54f0ae23d9136c40a7c6b428dd95b"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_csv('../input/train_V2.csv')\n",
    "train = reduce_mem_usage(train)\n",
    "test = pd.read_csv('../input/test_V2.csv')\n",
    "test = reduce_mem_usage(test)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "fbe4879dd44a73edffdc100d0158e551bf8edcb3"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ed20e8129574689a336d934a5ee6ecbabcfb0186"
   },
   "outputs": [],
   "source": [
    "null_cnt = train.isnull().sum().sort_values()\n",
    "print('null count:', null_cnt[null_cnt > 0])\n",
    "# dropna\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "10f263d9520b1041fea4184892e2c66ecbd7087f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.describe(include=np.number).drop('count').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c4c2f216b4a5844210786103c0b53325d2621f87"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f380c60d3e7f1d7c30ef85e003cef3e29044df74",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = train.append(test, sort=False).reset_index(drop=True)\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "669b837c115c19d61b40c79a7d3e7e72bb5e3474"
   },
   "source": [
    "## new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d077f08112b044cc360905361e366cdf1967271"
   },
   "outputs": [],
   "source": [
    "def fillInf(df, val):\n",
    "    numcols = df.select_dtypes(include='number').columns\n",
    "    cols = numcols[numcols != 'winPlacePerc']\n",
    "    df[df == np.Inf] = np.NaN\n",
    "    df[df == np.NINF] = np.NaN\n",
    "    for c in cols: df[c].fillna(val, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bc388eab6eaba7077d715460011bc26199be2cb"
   },
   "outputs": [],
   "source": [
    "all_data['_totalDistance'] = all_data['rideDistance'] + all_data['walkDistance'] + all_data['swimDistance']\n",
    "all_data['_healthItems'] = all_data['heals'] + all_data['boosts']\n",
    "all_data['_headshotKillRate'] = all_data['headshotKills'] / all_data['kills']\n",
    "all_data['_killPlaceOverMaxPlace'] = all_data['killPlace'] / all_data['maxPlace']\n",
    "all_data['_killsOverWalkDistance'] = all_data['kills'] / all_data['walkDistance']\n",
    "#all_data['_killsOverDistance'] = all_data['kills'] / all_data['_totalDistance']\n",
    "#all_data['_walkDistancePerSec'] = all_data['walkDistance'] / all_data['matchDuration']\n",
    "\n",
    "fillInf(all_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a463600c04fe5adb2d03c25eff03c7a7599f8d56"
   },
   "source": [
    "## rank as percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cee11a0d9059638278ac054edd0b352b75f7a644"
   },
   "outputs": [],
   "source": [
    "match = all_data.groupby('matchId')\n",
    "all_data['killsPerc'] = match['kills'].rank(pct=True).values\n",
    "all_data['killPlacePerc'] = match['killPlace'].rank(pct=True).values\n",
    "all_data['walkDistancePerc'] = match['walkDistance'].rank(pct=True).values\n",
    "#all_data['damageDealtPerc'] = match['damageDealt'].rank(pct=True).values\n",
    "\n",
    "all_data['walkPerc_killsPerc'] = all_data['walkDistancePerc'] / all_data['killsPerc']\n",
    "#all_data['walkPerc_kills'] = all_data['walkDistancePerc'] / all_data['kills']\n",
    "#all_data['kills_walkPerc'] = all_data['kills'] / all_data['walkDistancePerc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "26244216aa5995036da9485d3d8188031c43a10d"
   },
   "source": [
    "## drop feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1765f2581a9f07e9616ced120cb3aafd6611015c"
   },
   "outputs": [],
   "source": [
    "#all_data.drop(['killStreaks','DBNOs'], axis=1, inplace=True)\n",
    "all_data.drop(['boosts','heals','revives','assists'], axis=1, inplace=True)\n",
    "all_data.drop(['headshotKills','roadKills','vehicleDestroys','teamKills'], axis=1, inplace=True)\n",
    "all_data.drop(['rideDistance','swimDistance','matchDuration'], axis=1, inplace=True)\n",
    "all_data.drop(['rankPoints','killPoints','winPoints'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f9ecb5ec4de85e60fde11b1fa9a8e8ec850094c6"
   },
   "source": [
    "## grouping\n",
    "\n",
    "* need to predict the order of places for groups within each match.\n",
    "* train on group-level instead of the user-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72b532cda47ca292bd0bfd5e2982f5d9aeab3401"
   },
   "outputs": [],
   "source": [
    "match = all_data.groupby(['matchId'])\n",
    "group = all_data.groupby(['matchId','groupId','matchType'])\n",
    "\n",
    "# target feature (max, min)\n",
    "agg_col = list(all_data.columns)\n",
    "exclude_agg_col = ['Id','matchId','groupId','matchType','maxPlace','numGroups','winPlacePerc']\n",
    "for c in exclude_agg_col:\n",
    "    agg_col.remove(c)\n",
    "print(agg_col)\n",
    "\n",
    "# target feature (sum)\n",
    "sum_col = ['kills','killPlace','damageDealt','walkDistance','_healthItems']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5318ce2a99f2960fef819a9793d939032e52c844"
   },
   "outputs": [],
   "source": [
    "''' match sum, match max, match mean, group sum\n",
    "'''\n",
    "match_data = pd.concat([\n",
    "    match.size().to_frame('m.players'), \n",
    "    match[sum_col].sum().rename(columns=lambda s: 'm.sum.' + s), \n",
    "    match[sum_col].max().rename(columns=lambda s: 'm.max.' + s),\n",
    "    match[sum_col].mean().rename(columns=lambda s: 'm.mean.' + s)\n",
    "    ], axis=1).reset_index()\n",
    "match_data = pd.merge(match_data, \n",
    "    group[sum_col].sum().rename(columns=lambda s: 'sum.' + s).reset_index())\n",
    "match_data = reduce_mem_usage(match_data)\n",
    "\n",
    "print(match_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "270cbffd8766c34043d209258e64dbaee02a0a96"
   },
   "outputs": [],
   "source": [
    "''' ranking of kills and killPlace in each match\n",
    "'''\n",
    "minKills = all_data.sort_values(['matchId','groupId','kills','killPlace']).groupby(\n",
    "    ['matchId','groupId','kills']).first().reset_index().copy()\n",
    "for n in np.arange(5):\n",
    "    c = 'kills_' + str(n) + '_Place'\n",
    "    nKills = (minKills['kills'] == n)\n",
    "    minKills.loc[nKills, c] = minKills[nKills].groupby(['matchId'])['killPlace'].rank().values\n",
    "    match_data = pd.merge(match_data, minKills[nKills][['matchId','groupId',c]], how='left')\n",
    "    match_data[c].fillna(0, inplace=True)\n",
    "match_data = reduce_mem_usage(match_data)\n",
    "del minKills, nKills\n",
    "\n",
    "print(match_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ab91d60f601ab7bc4a997de22a7e60a0cd5ea07"
   },
   "outputs": [],
   "source": [
    "''' group mean, max, min\n",
    "'''\n",
    "all_data = pd.concat([\n",
    "    group.size().to_frame('players'),\n",
    "    group.mean(),\n",
    "    group[agg_col].max().rename(columns=lambda s: 'max.' + s),\n",
    "    group[agg_col].min().rename(columns=lambda s: 'min.' + s),\n",
    "    ], axis=1).reset_index()\n",
    "all_data = reduce_mem_usage(all_data)\n",
    "\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbe45939855acb980439ea2f83ed7c6f2d87e95d"
   },
   "source": [
    "## aggregate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4bc443565321a07a9ce0c161ec8f9924b83d2d5"
   },
   "outputs": [],
   "source": [
    "numcols = all_data.select_dtypes(include='number').columns.values\n",
    "numcols = numcols[numcols != 'winPlacePerc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2dac1842b61335675367c7a4ac180c6b3dbc42bb"
   },
   "outputs": [],
   "source": [
    "''' match summary, max\n",
    "'''\n",
    "all_data = pd.merge(all_data, match_data)\n",
    "del match_data\n",
    "gc.collect()\n",
    "\n",
    "all_data['enemy.players'] = all_data['m.players'] - all_data['players']\n",
    "for c in sum_col:\n",
    "    all_data['enemy.' + c] = (all_data['m.sum.' + c] - all_data['sum.' + c]) / all_data['enemy.players']\n",
    "    #all_data['p.sum_msum.' + c] = all_data['sum.' + c] / all_data['m.sum.' + c]\n",
    "    #all_data['p.max_mmean.' + c] = all_data['max.' + c] / all_data['m.mean.' + c]\n",
    "    all_data['p.max_msum.' + c] = all_data['max.' + c] / all_data['m.sum.' + c]\n",
    "    all_data['p.max_mmax.' + c] = all_data['max.' + c] / all_data['m.max.' + c]\n",
    "    all_data.drop(['m.sum.' + c, 'm.max.' + c], axis=1, inplace=True)\n",
    "    \n",
    "fillInf(all_data, 0)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d63ccf63773c949805683f5aee3c3699f1f3aeb7"
   },
   "outputs": [],
   "source": [
    "''' match rank\n",
    "'''\n",
    "match = all_data.groupby('matchId')\n",
    "matchRank = match[numcols].rank(pct=True).rename(columns=lambda s: 'rank.' + s)\n",
    "all_data = reduce_mem_usage(pd.concat([all_data, matchRank], axis=1))\n",
    "rank_col = matchRank.columns\n",
    "del matchRank\n",
    "gc.collect()\n",
    "\n",
    "# instead of rank(pct=True, method='dense')\n",
    "match = all_data.groupby('matchId')\n",
    "matchRank = match[rank_col].max().rename(columns=lambda s: 'max.' + s).reset_index()\n",
    "all_data = pd.merge(all_data, matchRank)\n",
    "for c in numcols:\n",
    "    all_data['rank.' + c] = all_data['rank.' + c] / all_data['max.rank.' + c]\n",
    "    all_data.drop(['max.rank.' + c], axis=1, inplace=True)\n",
    "del matchRank\n",
    "gc.collect()\n",
    "\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd2cfbf18321ea2e8d64c47a392c70c10556b7a2"
   },
   "source": [
    "## killPlace rank of group and kills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a70258951066929394045448888ed0b5396d6c84"
   },
   "outputs": [],
   "source": [
    "''' TODO: incomplete\n",
    "''' \n",
    "killMinorRank = all_data[['matchId','min.kills','max.killPlace']].copy()\n",
    "group = killMinorRank.groupby(['matchId','min.kills'])\n",
    "killMinorRank['rank.minor.maxKillPlace'] = group.rank(pct=True).values\n",
    "all_data = pd.merge(all_data, killMinorRank)\n",
    "\n",
    "killMinorRank = all_data[['matchId','max.kills','min.killPlace']].copy()\n",
    "group = killMinorRank.groupby(['matchId','max.kills'])\n",
    "killMinorRank['rank.minor.minKillPlace'] = group.rank(pct=True).values\n",
    "all_data = pd.merge(all_data, killMinorRank)\n",
    "\n",
    "del killMinorRank\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ff6408d747855db53b14d6872aff2ba90521be3"
   },
   "source": [
    "## drop constant feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e65a2de00d692f724ae1eaec6f9ef30ff9e9663"
   },
   "outputs": [],
   "source": [
    "# drop constant column\n",
    "constant_column = [col for col in all_data.columns if all_data[col].nunique() == 1]\n",
    "print('drop columns:', constant_column)\n",
    "all_data.drop(constant_column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4260c2df241609760f64056be3bd01615bae2a95"
   },
   "source": [
    "## encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ebd2851bb4f66edea40eeaf50042bb18a5379988"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "solo  <-- solo,solo-fpp,normal-solo,normal-solo-fpp\n",
    "duo   <-- duo,duo-fpp,normal-duo,normal-duo-fpp,crashfpp,crashtpp\n",
    "squad <-- squad,squad-fpp,normal-squad,normal-squad-fpp,flarefpp,flaretpp\n",
    "'''\n",
    "mapper = lambda x: 'solo' if ('solo' in x) else 'duo' if ('duo' in x) or ('crash' in x) else 'squad'\n",
    "all_data['matchType'] = all_data['matchType'].apply(mapper)\n",
    "\n",
    "all_data = pd.concat([all_data, pd.get_dummies(all_data['matchType'], prefix='matchType')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7138c7995f98a6704d0f9952774917a075b7c14"
   },
   "outputs": [],
   "source": [
    "cols = [col for col in all_data.columns if col not in ['Id','matchId','groupId']]\n",
    "for i, t in all_data.loc[:, cols].dtypes.iteritems():\n",
    "    if t == object:\n",
    "        all_data[i] = pd.factorize(all_data[i])[0]\n",
    "\n",
    "all_data = reduce_mem_usage(all_data)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8db28f0dcc9d2a0de93238de1a6eb76bb7f6331b"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9638c50680633d176596da0dbb085ca31bacf19"
   },
   "outputs": [],
   "source": [
    "X_train = all_data[all_data['winPlacePerc'].notnull()].reset_index(drop=True)\n",
    "X_test = all_data[all_data['winPlacePerc'].isnull()].drop(['winPlacePerc'], axis=1).reset_index(drop=True)\n",
    "del all_data\n",
    "gc.collect()\n",
    "\n",
    "Y_train = X_train.pop('winPlacePerc')\n",
    "X_test_grp = X_test[['matchId','groupId']].copy()\n",
    "\n",
    "# drop matchId,groupId\n",
    "X_train.drop(['matchId','groupId'], axis=1, inplace=True)\n",
    "X_test.drop(['matchId','groupId'], axis=1, inplace=True)\n",
    "\n",
    "X_train_cols = X_train.columns\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e777c101f7ce2cec11e284daabd4c1f94edae41d"
   },
   "outputs": [],
   "source": [
    "#print(pd.DataFrame([[val for val in dir()], [sys.getsizeof(eval(val)) for val in dir()]],\n",
    "#                   index=['name','size']).T.sort_values('size', ascending=False).reset_index(drop=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "725a31e7756fa677e26a5b4bd48ff756222f3513"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, PReLU\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, kernel_initializer='he_normal', input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256, kernel_initializer='he_normal'))\n",
    "    model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(128, kernel_initializer='he_normal'))\n",
    "    model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "    optimizer = optimizers.Adam(lr=0.005)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    #model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f67a96ebb513470dfb5ba116400eaaed38d14103"
   },
   "outputs": [],
   "source": [
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n",
    "    ''' Wrapper function to create a LearningRateScheduler with step decay schedule. '''\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "    \n",
    "    return LearningRateScheduler(schedule, verbose)\n",
    "\n",
    "lr_sched = step_decay_schedule(initial_lr=0.001, decay_factor=0.97, step_size=1, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_mean_absolute_error', mode='min', patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc48e7700f42ba20351ac2ef4900fb8909cf6f6a"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from tensorflow import set_random_seed\n",
    "np.random.seed(42)\n",
    "set_random_seed(1234)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train.astype(float))\n",
    "X_train = scaler.transform(X_train.astype(float))\n",
    "X_test = scaler.transform(X_test.astype(float))\n",
    "\n",
    "model = createModel()\n",
    "history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        epochs=200,\n",
    "        batch_size=2**15,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[lr_sched, early_stopping],\n",
    "        verbose=2)\n",
    "pred = model.predict(X_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dcbae65430ef43f08591ccd200fe33f802d938a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation mae values\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('Mean Abosulte Error')\n",
    "plt.ylabel('Mean absolute error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "914305f240aa0ca6d0c6cd16954ccab4815e4bcb"
   },
   "source": [
    "## feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d30d3517efebdb6df8e2164b3ce36a879b6aa358"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "\n",
    "def score(X, y):\n",
    "    y_pred = model.predict(X).ravel()\n",
    "    return np.sum(np.abs(y - y_pred))\n",
    "\n",
    "base_score, score_decreases = get_score_importances(score, X_train[:10000], Y_train[:10000])\n",
    "feature_importances = np.mean(score_decreases, axis=0) * -1\n",
    "\n",
    "feature_importances = 100.0 * (feature_importances / feature_importances.max())\n",
    "sorted_idx = np.argsort(feature_importances)\n",
    "sorted_idx = sorted_idx[len(feature_importances) - 30:]\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.barh(pos, feature_importances[sorted_idx], align='center')\n",
    "plt.yticks(pos, X_train_cols[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()\n",
    "\n",
    "X_train_cols[np.argsort(feature_importances)[::-1]].values\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f8e666941a6157dda493f36c8084e4f021ff454"
   },
   "source": [
    "## alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "605701ae5d89d21ac56ac3ddcf69175f936e3b18"
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('../input/test_V2.csv')\n",
    "X_test = X_test.groupby(['matchId','groupId','matchType']).first().reset_index()\n",
    "X_test = X_test[['matchId','groupId','matchType','numGroups','maxPlace','kills','killPlace']]\n",
    "\n",
    "group = X_test_grp.groupby(['matchId'])\n",
    "X_test_grp['winPlacePerc'] = pred\n",
    "X_test_grp['_rank.winPlacePerc'] = group['winPlacePerc'].rank(method='min')\n",
    "X_test = pd.merge(X_test, X_test_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae29276c07cdd5c255205eb4cae731bad1596521",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fullgroup = (X_test['numGroups'] == X_test['maxPlace'])\n",
    "\n",
    "# full group (201366) --> calculate from rank\n",
    "subset = X_test.loc[fullgroup]\n",
    "X_test.loc[fullgroup, 'winPlacePerc'] = (subset['_rank.winPlacePerc'].values - 1) / (subset['maxPlace'].values - 1)\n",
    "\n",
    "# not full group (684872) --> align with maxPlace\n",
    "subset = X_test.loc[~fullgroup]\n",
    "gap = 1.0 / (subset['maxPlace'].values - 1)\n",
    "new_perc = np.around(subset['winPlacePerc'].values / gap) * gap  # half&up\n",
    "X_test.loc[~fullgroup, 'winPlacePerc'] = new_perc\n",
    "\n",
    "X_test['winPlacePerc'] = X_test['winPlacePerc'].clip(lower=0,upper=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e95c4aab16d2954542f986adacbe2e8392037d3"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# credit to https://www.kaggle.com/nagroda100/pubg-submission-postprocessor/code\n",
    "print(\"Checking for anomalies in the winPlacePerc - players with same number of kills should have scores in order of killPlace\")\n",
    "\n",
    "do_correct = True\n",
    "iteration_number = 1\n",
    "\n",
    "while do_correct & (iteration_number <= 1000):\n",
    "    X_test.sort_values(ascending=False, by=[\"matchId\",\"kills\",\"killPlace\",\"winPlacePerc\",\"groupId\"], inplace=True)\n",
    "    X_test[\"winPlacePerc_diff\"] = X_test[\"winPlacePerc\"].diff()\n",
    "    X_test[\"kills_diff\"] = X_test[\"kills\"].diff()\n",
    "    X_test[\"prev_matchId\"] = X_test[\"matchId\"].shift(1)\n",
    "    X_test[\"prev_groupId\"] = X_test[\"groupId\"].shift(1)\n",
    "    X_test[\"prev_winPlacePerc\"] = X_test[\"winPlacePerc\"].shift(1)\n",
    "\n",
    "    df_sub2 = X_test[(X_test[\"winPlacePerc_diff\"] < 0) \n",
    "                     & (X_test[\"kills_diff\"] == 0) \n",
    "                     & (X_test[\"matchId\"] == X_test[\"prev_matchId\"])]\n",
    "    anomalies_count = len(df_sub2)\n",
    "\n",
    "    print(\"Iteration \" + str(iteration_number) + \" Anomalies count: \" + str(anomalies_count))\n",
    "\n",
    "    changed_groups = list()\n",
    "\n",
    "    if anomalies_count > 0:\n",
    "        print()\n",
    "        print(\"Looking for pairs to change...\")\n",
    "\n",
    "        df_sub2[\"new_winPlacePerc\"] = df_sub2[\"winPlacePerc\"] \n",
    "\n",
    "        df_sub3 = pd.DataFrame()\n",
    "\n",
    "        for i in tqdm(range(1, min(15001, max(anomalies_count, 2))), \n",
    "                      desc=\"Identifying unique groups\", mininterval=10):\n",
    "            row = df_sub2.iloc[i - 1]\n",
    "            id_prev = str(row[\"prev_matchId\"]) + \"!\" + str(row[\"prev_groupId\"])\n",
    "            id_cur = str(row[\"matchId\"]) + \"!\" + str(row[\"groupId\"])\n",
    "            if (not id_prev in changed_groups) & (not id_cur in changed_groups):\n",
    "                changed_groups.append(id_prev)\n",
    "                changed_groups.append(id_cur)\n",
    "                df_sub3 = df_sub3.append({\"matchId\": row[\"matchId\"], \"groupId\": row[\"prev_groupId\"], \n",
    "                                          \"new_winPlacePerc\": row[\"winPlacePerc\"]}, \n",
    "                                         sort=False, ignore_index=True)\n",
    "                df_sub3 = df_sub3.append({\"matchId\": row[\"matchId\"], \"groupId\": row[\"groupId\"], \n",
    "                                          \"new_winPlacePerc\": row[\"prev_winPlacePerc\"]}, \n",
    "                                         sort=False, ignore_index=True)\n",
    "\n",
    "        df_sub3.drop_duplicates(inplace=True)\n",
    "        X_test = X_test.merge(df_sub3, on=[\"matchId\", \"groupId\"], how=\"left\")\n",
    "        notna = X_test[\"new_winPlacePerc\"].notna()\n",
    "        X_test.loc[notna, \"winPlacePerc\"] = X_test.loc[notna][\"new_winPlacePerc\"]\n",
    "        X_test.drop(labels=\"new_winPlacePerc\", axis=1, inplace=True)\n",
    "        del df_sub2\n",
    "        del df_sub3\n",
    "        df_sub2 = None\n",
    "        df_sub3 = None\n",
    "        gc.collect()\n",
    "    else:\n",
    "        do_correct = False\n",
    "\n",
    "    iteration_number = iteration_number + 1\n",
    "\n",
    "if do_correct:\n",
    "    print(\"Limit of iterations reached...\")\n",
    "\n",
    "print(\"Finished correcting winPlacePerc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ab07f991a8d6cd17ba36c0510a2a507acb808f4"
   },
   "outputs": [],
   "source": [
    "# edge cases\n",
    "X_test.loc[X_test['maxPlace'] == 0, 'winPlacePerc'] = 0\n",
    "X_test.loc[X_test['maxPlace'] == 1, 'winPlacePerc'] = 1  # nothing\n",
    "X_test.loc[(X_test['maxPlace'] > 1) & (X_test['numGroups'] == 1), 'winPlacePerc'] = 0\n",
    "X_test['winPlacePerc'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "86209503fc7f68f37024f82057cf10819ef75634"
   },
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cd3bf14e3d8d03d9ebfa45826acbbb38cb421e4"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/test_V2.csv')\n",
    "\n",
    "submission = pd.merge(test, X_test[['matchId','groupId','winPlacePerc']])\n",
    "submission = submission[['Id','winPlacePerc']]\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2527e0ed0735c2b09a7f00880265d19da515eb74"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
