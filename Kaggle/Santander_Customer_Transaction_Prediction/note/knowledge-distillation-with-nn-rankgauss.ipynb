{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3108ed389c051980f763a7b75970c95f4cd4d97"
   },
   "source": [
    "# Introduction\n",
    "**\"[Distilling the Knowledge in a Neural Network](http://arxiv.org/abs/1503.02531)\" was introduced by Geoffrey Hinton, Oriol Vinyals, Jeff Dean in Mar 2015. In this kernel, I would like to share some experiments to distill the knowledge from a [LGBM teacher](https://www.kaggle.com/tanreinama/lightgbm-minimize-leaves-with-gaussiannb) (LB:0.899) to a neural network. The student network has not surpassed the teacher model yet (LB:0.894). But, I hope I can make it happen before this competition ends.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33da2d6967f8899638219ae1698411ada9d02b7e"
   },
   "source": [
    "# Please upvote if you find this kernel interesting ^_^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['santander-customer-transaction-prediction', 'santander-2019-distillation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "# np.random.seed(8)\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, scale\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "from keras.losses import binary_crossentropy\n",
    "import gc\n",
    "import scipy.special\n",
    "from tqdm import *\n",
    "from scipy.stats import norm, rankdata\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "NUM_FEATURES = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b6e65eca995c134ad532f7a8acb7b106575dde9"
   },
   "source": [
    "## **Load the dataset, and the prediction of 5-fold LGBM**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "90cce1d9df7f0a4a838bdbe1ae3e190a9ecd7147"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\n",
    "test = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\n",
    "train_knowledge = pd.read_csv('../input/santander-2019-distillation/lgbm_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "3c34513f569d5fa7fa79c39f995f09b555c2e2bf"
   },
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "y_knowledge = train_knowledge['target']\n",
    "id_code_train = train['ID_code']\n",
    "id_code_test = test['ID_code']\n",
    "features = [c for c in train.columns if c not in ['ID_code', 'target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7f26e493b56fe58fdf82fe2936bff8b5c61a846"
   },
   "source": [
    "## Adding some features, the credit belong to these kernels: https://www.kaggle.com/karangautam/keras-nn, https://www.kaggle.com/ymatioun/santander-linear-model-with-additional-features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "ac0a1aa3268892879ccb3ebc9e36e39a4612c0dd"
   },
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    # train['mean_'+feature] = (train[feature].mean()-train[feature])\n",
    "    # train['z_'+feature] = (train[feature] - train[feature].mean())/train[feature].std(ddof=0)\n",
    "    train['sq_'+feature] = (train[feature])**2\n",
    "    # train['sqrt_'+feature] = np.abs(train[feature])**(1/2)\n",
    "    train['c_'+feature] = (train[feature])**3\n",
    "    # train['p4_'+feature] = (train[feature])**4\n",
    "    # train['r1_'+feature] = np.round(train[feature], 1)\n",
    "    train['r2_'+feature] = np.round(train[feature], 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7624675031843a1c065c982ca501e95a45e4ef90"
   },
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    # test['mean_'+feature] = (train[feature].mean()-test[feature])\n",
    "    # test['z_'+feature] = (test[feature] - train[feature].mean())/train[feature].std(ddof=0)\n",
    "    test['sq_'+feature] = (test[feature])**2\n",
    "    # test['sqrt_'+feature] = np.abs(test[feature])**(1/2)\n",
    "    test['c_'+feature] = (test[feature])**3\n",
    "    # test['p4_'+feature] = (test[feature])**4\n",
    "    # test['r1_'+feature] = np.round(test[feature], 1)\n",
    "    test['r2_'+feature] = np.round(test[feature], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a98a587502fef9575b74f94dd7e4f41188ab4b37"
   },
   "source": [
    "## Normalize and split data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "b631bca7d0fcacc45ca7f35e14717619843e46c5"
   },
   "outputs": [],
   "source": [
    "class GaussRankScaler():\n",
    "\n",
    "    def __init__( self ):\n",
    "        self.epsilon = 1e-9\n",
    "        self.lower = -1 + self.epsilon\n",
    "        self.upper =  1 - self.epsilon\n",
    "        self.range = self.upper - self.lower\n",
    "\n",
    "    def fit_transform( self, X ):\n",
    "\n",
    "        i = np.argsort( X, axis = 0 )\n",
    "        j = np.argsort( i, axis = 0 )\n",
    "\n",
    "        assert ( j.min() == 0 ).all()\n",
    "        assert ( j.max() == len( j ) - 1 ).all()\n",
    "\n",
    "        j_range = len( j ) - 1\n",
    "        self.divider = j_range / self.range\n",
    "\n",
    "        transformed = j / self.divider\n",
    "        transformed = transformed - self.upper\n",
    "        transformed = scipy.special.erfinv( transformed )\n",
    "        ############\n",
    "        # transformed = transformed - np.mean(transformed)\n",
    "\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "108750758b3cd9eeec6fd646585b73af93b162ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n",
      "100%|██████████| 200/200 [00:25<00:00,  7.88it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 174.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n",
      "(200000, 1200)\n"
     ]
    }
   ],
   "source": [
    "SPLIT = len(train)\n",
    "train = train.append(test)\n",
    "del test; gc.collect()\n",
    "# print(train.shape)\n",
    "scaler = GaussRankScaler()\n",
    "sc = StandardScaler()\n",
    "for feat in tqdm(features):\n",
    "    # train[feat] = scaler.fit_transform(train[feat])\n",
    "    train[feat] = sc.fit_transform(train[feat].values.reshape(-1, 1))\n",
    "    train[feat+'_r'] = rankdata(train[feat]).astype('float32')\n",
    "    train[feat+'_n'] = norm.cdf(train[feat]).astype('float32')\n",
    "\n",
    "feats = [c for c in train.columns if c not in (['ID_code', 'target'] + features)]\n",
    "for feat in tqdm(feats):\n",
    "    train[feat] = sc.fit_transform(train[feat].values.reshape(-1, 1))\n",
    "\n",
    "train = train.drop(['target', 'ID_code'], axis=1)\n",
    "test = train[SPLIT:].values\n",
    "train = train[:SPLIT].values\n",
    "# test = test.drop(['ID_code'], axis=1)\n",
    "print('Done!!')\n",
    "print(train.shape)\n",
    "# train.head()\n",
    "# train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "6f3998f86595789305d3fec999bbc76adc7971a1"
   },
   "outputs": [],
   "source": [
    "train = np.reshape(train, (-1, NUM_FEATURES, 1))\n",
    "test = np.reshape(test, (-1, NUM_FEATURES, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "3f3220d0506c8e89ce8a99d1cc8fe77e83275a17"
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid  = train_test_split(train, y, y_knowledge, stratify=y, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51a4ef6c1d48eb4d83f18248256bf2884f02504c"
   },
   "source": [
    "## Define our student network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "09e980734152207ef1a7cec18d717733e40edfba"
   },
   "outputs": [],
   "source": [
    "function = 'relu'\n",
    "# function = keras.layers.advanced_activations.LeakyReLU(alpha=.001)\n",
    "\n",
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    x = Dense(16, activation=function)(input_tensor)\n",
    "    x = Flatten()(x)\n",
    "    out_put = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, out_put)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4b5cc0565c0c1ab30cf2958e98468e4493f0189"
   },
   "source": [
    "## Some necessary functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "5cd85490ab33a4e8583427955fc29a1de218132b"
   },
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "d8ad256648019aa6908bdda89245d24b2865564a"
   },
   "outputs": [],
   "source": [
    "gamma = 2.0\n",
    "alpha=.25\n",
    "epsilon = K.epsilon()\n",
    "\n",
    "def focal_loss(y_true, y_pred):\n",
    "    pt_1 = y_pred * y_true\n",
    "    pt_1 = K.clip(pt_1, epsilon, 1-epsilon)\n",
    "    CE_1 = -K.log(pt_1)\n",
    "    FL_1 = alpha* K.pow(1-pt_1, gamma) * CE_1\n",
    "    \n",
    "    pt_0 = (1-y_pred) * (1-y_true)\n",
    "    pt_0 = K.clip(pt_0, epsilon, 1-epsilon)\n",
    "    CE_0 = -K.log(pt_0)\n",
    "    FL_0 = (1-alpha)* K.pow(1-pt_0, gamma) * CE_0\n",
    "    \n",
    "    loss = K.sum(FL_1, axis=1) + K.sum(FL_0, axis=1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "51906f2d2a1ce3d937df6d401c831cd58a7f9b4e"
   },
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    # y = np.array(y)\n",
    "    # print(y)\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    sample_size = x.shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    np.random.shuffle(index_array)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index_array]\n",
    "    mixed_y = (lam * y) + ((1 - lam) * y[index_array])\n",
    "    # print((1 - lam) * y[index_array])\n",
    "    # print((lam * y).shape,((1 - lam) * y[index_array]).shape)\n",
    "    return mixed_x, mixed_y\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n",
    "\n",
    "\n",
    "def batch_generator(X,y,batch_size=128,shuffle=True,mixup=False):\n",
    "    y = np.array(y)\n",
    "    # print(X.shape[0], y.shape[0])\n",
    "    sample_size = X.shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    \n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index_array)\n",
    "        batches = make_batches(sample_size, batch_size)\n",
    "        for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "            batch_ids = index_array[batch_start:batch_end]\n",
    "            X_batch = X[batch_ids]\n",
    "            y_batch = y[batch_ids]\n",
    "            \n",
    "            if mixup:\n",
    "                # print('before', X_batch.shape, y_batch.shape)\n",
    "                X_batch,y_batch = mixup_data(X_batch,y_batch,alpha=1.0)\n",
    "            # print('*****************')    \n",
    "            yield X_batch,y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "703fe0fa610133323f862f412c8e22c3beaf0ad2"
   },
   "source": [
    "## Experiment 1\n",
    "Firstly, we check the performance of simple feed forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "3c657dc6f052fb182cc46027283ae2929dd7a014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1200, 1)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1200, 16)          32        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 19201     \n",
      "=================================================================\n",
      "Total params: 19,233\n",
      "Trainable params: 19,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.2924 - val_loss: 0.2332\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.23325, saving model to feed_forward_model.h5\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 4s 24ms/step - loss: 0.2685 - val_loss: 0.2422\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.23325\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 4s 24ms/step - loss: 0.2591 - val_loss: 0.2220\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23325 to 0.22199, saving model to feed_forward_model.h5\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 4s 23ms/step - loss: 0.2560 - val_loss: 0.2218\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.22199 to 0.22182, saving model to feed_forward_model.h5\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 4s 23ms/step - loss: 0.2549 - val_loss: 0.2157\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.22182 to 0.21571, saving model to feed_forward_model.h5\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 4s 23ms/step - loss: 0.2535 - val_loss: 0.2160\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.21571\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 4s 24ms/step - loss: 0.2522 - val_loss: 0.2166\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21571\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 4s 24ms/step - loss: 0.2514 - val_loss: 0.2174\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21571\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 4s 24ms/step - loss: 0.2513 - val_loss: 0.2107\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.21571 to 0.21065, saving model to feed_forward_model.h5\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 4s 24ms/step - loss: 0.2491 - val_loss: 0.2122\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.21065\n"
     ]
    }
   ],
   "source": [
    "# model = create_model((train.shape[1],), 1)\n",
    "model = create_model((NUM_FEATURES,1), 1)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint('feed_forward_model.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='min', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=9)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "tr_gen = batch_generator(x_train,y_train,batch_size=BATCH_SIZE,shuffle=True,mixup=True)\n",
    "\n",
    "history = model.fit_generator(# x_train,y_train,\n",
    "                                tr_gen,\n",
    "                                steps_per_epoch=np.ceil(float(len(x_train)) / float(BATCH_SIZE)),\n",
    "                                epochs=10,\n",
    "                                verbose=1,\n",
    "                                callbacks=callbacks_list,\n",
    "                                validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3d024753128c64a5d31a6d861bcb1d7f38c65366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8888311119499778"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('feed_forward_model.h5')\n",
    "prediction = model.predict(x_valid, batch_size=512, verbose=1)\n",
    "roc_auc_score(y_valid, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc3a1cf3d0df24df14f636f863f1bc67aac7659f"
   },
   "source": [
    "## Knowledge distillation\n",
    "The basic idea is that you feed both groundtruth and the prediction from the teacher model to the student network.\n",
    "Soft targets (the prediction of the teacher model) contains more information than the hard labels (groundtruth) due to the fact that they encode similarity measures between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "4bc6654afc5e5a834efc5ca09a44605eb62b6bee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00560906])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.vstack((y_train, y_knowledge_train)).T\n",
    "y_valid = np.vstack((y_valid, y_knowledge_valid)).T\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "a70bf72aca056687f154690347a66bba3f8026d2"
   },
   "outputs": [],
   "source": [
    "def knowledge_distillation_loss_withBE(y_true, y_pred, beta=0.1):\n",
    "\n",
    "    # Extract the groundtruth from dataset and the prediction from teacher model\n",
    "    y_true, y_pred_teacher = y_true[: , :1], y_true[: , 1:]\n",
    "    \n",
    "    # Extract the prediction from student model\n",
    "    y_pred, y_pred_stu = y_pred[: , :1], y_pred[: , 1:]\n",
    "\n",
    "    loss = beta*binary_crossentropy(y_true,y_pred) + (1-beta)*binary_crossentropy(y_pred_teacher, y_pred_stu)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "6452176b0ffc214ea09c24c8b5a83a2ffd102c51"
   },
   "outputs": [],
   "source": [
    "def auc_2(y_true, y_pred):\n",
    "    y_true = y_true[:, :1]\n",
    "    y_pred = y_pred[:, :1]\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "def auc_3(y_true, y_pred):\n",
    "    y_true = y_true[:, :1]\n",
    "    y_pred = y_pred[:, 1:]\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be939049ab91a59440245cc4243561fd60e4bc4c"
   },
   "source": [
    "## Experment 2\n",
    "We set the ratio between teacher's prediction and groundtruth is 1:9, and use the basic binary crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "c3b91e105d49537cae2d1e43294ffa4483e4298f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/10\n",
      "160000/160000 [==============================] - 4s 24us/step - loss: 0.2784 - auc_2: 0.7996 - val_loss: 0.2435 - val_auc_2: 0.8532\n",
      "Epoch 2/10\n",
      "160000/160000 [==============================] - 3s 21us/step - loss: 0.2355 - auc_2: 0.8676 - val_loss: 0.2326 - val_auc_2: 0.8711\n",
      "Epoch 3/10\n",
      "160000/160000 [==============================] - 3s 21us/step - loss: 0.2284 - auc_2: 0.8788 - val_loss: 0.2279 - val_auc_2: 0.8769\n",
      "Epoch 4/10\n",
      "160000/160000 [==============================] - 3s 21us/step - loss: 0.2249 - auc_2: 0.8841 - val_loss: 0.2251 - val_auc_2: 0.8814\n",
      "Epoch 5/10\n",
      "160000/160000 [==============================] - 3s 20us/step - loss: 0.2230 - auc_2: 0.8872 - val_loss: 0.2236 - val_auc_2: 0.8825\n",
      "Epoch 6/10\n",
      "160000/160000 [==============================] - 3s 20us/step - loss: 0.2218 - auc_2: 0.8890 - val_loss: 0.2243 - val_auc_2: 0.8849\n",
      "Epoch 7/10\n",
      "160000/160000 [==============================] - 3s 21us/step - loss: 0.2211 - auc_2: 0.8902 - val_loss: 0.2234 - val_auc_2: 0.8855\n",
      "Epoch 8/10\n",
      "160000/160000 [==============================] - 3s 21us/step - loss: 0.2204 - auc_2: 0.8915 - val_loss: 0.2220 - val_auc_2: 0.8858\n",
      "Epoch 9/10\n",
      "160000/160000 [==============================] - 3s 20us/step - loss: 0.2199 - auc_2: 0.8924 - val_loss: 0.2213 - val_auc_2: 0.8856\n",
      "Epoch 10/10\n",
      "160000/160000 [==============================] - 3s 20us/step - loss: 0.2196 - auc_2: 0.8929 - val_loss: 0.2208 - val_auc_2: 0.8875\n"
     ]
    }
   ],
   "source": [
    "# model = create_model((train.shape[1],), 2)\n",
    "model = create_model((NUM_FEATURES,1), 2)\n",
    "model.compile(loss=knowledge_distillation_loss_withBE, optimizer='adam', metrics=[auc_2])\n",
    "\n",
    "checkpoint = ModelCheckpoint('student_model_BE.h5', monitor='val_auc_2', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_auc_2\", \n",
    "                      mode=\"max\", \n",
    "                      patience=9)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "\n",
    "history = model.fit(x_train,y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efa0411e6495f5ff309650a6d77ddeca65948278"
   },
   "source": [
    "## Experment 3\n",
    "We set the ratio between teacher's prediction and groundtruth is 1:9, and use the focal loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "442f656984e998fc9d425b19736936c6db73bd43"
   },
   "outputs": [],
   "source": [
    "def knowledge_distillation_loss_withFL(y_true, y_pred, beta=0.1):\n",
    "\n",
    "    # Extract the groundtruth from dataset and the prediction from teacher model\n",
    "    y_true, y_pred_teacher = y_true[: , :1], y_true[: , 1:]\n",
    "    \n",
    "    # Extract the prediction from student model\n",
    "    y_pred, y_pred_stu = y_pred[: , :1], y_pred[: , 1:]\n",
    "\n",
    "    loss = beta*focal_loss(y_true,y_pred) + (1-beta)*binary_crossentropy(y_pred_teacher, y_pred_stu)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "6641a36d26185a8b98fc4b7001a18814951e4101"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/10\n",
      "160000/160000 [==============================] - 4s 27us/step - loss: 0.7404 - auc_2: 0.7912 - auc_3: 0.7981 - val_loss: 0.7034 - val_auc_2: 0.8611 - val_auc_3: 0.8632\n",
      "\n",
      "Epoch 00001: val_auc_2 improved from -inf to 0.86112, saving model to student_model_FL.h5\n",
      "Epoch 2/10\n",
      "160000/160000 [==============================] - 4s 23us/step - loss: 0.6979 - auc_2: 0.8730 - auc_3: 0.8740 - val_loss: 0.6962 - val_auc_2: 0.8747 - val_auc_3: 0.8766\n",
      "\n",
      "Epoch 00002: val_auc_2 improved from 0.86112 to 0.87473, saving model to student_model_FL.h5\n",
      "Epoch 3/10\n",
      "160000/160000 [==============================] - 4s 23us/step - loss: 0.6924 - auc_2: 0.8811 - auc_3: 0.8841 - val_loss: 0.6919 - val_auc_2: 0.8785 - val_auc_3: 0.8837\n",
      "\n",
      "Epoch 00003: val_auc_2 improved from 0.87473 to 0.87854, saving model to student_model_FL.h5\n",
      "Epoch 4/10\n",
      "160000/160000 [==============================] - 4s 23us/step - loss: 0.6892 - auc_2: 0.8858 - auc_3: 0.8896 - val_loss: 0.6898 - val_auc_2: 0.8791 - val_auc_3: 0.8875\n",
      "\n",
      "Epoch 00004: val_auc_2 improved from 0.87854 to 0.87913, saving model to student_model_FL.h5\n",
      "Epoch 5/10\n",
      "160000/160000 [==============================] - 4s 23us/step - loss: 0.6876 - auc_2: 0.8860 - auc_3: 0.8927 - val_loss: 0.6892 - val_auc_2: 0.8810 - val_auc_3: 0.8898\n",
      "\n",
      "Epoch 00005: val_auc_2 improved from 0.87913 to 0.88097, saving model to student_model_FL.h5\n",
      "Epoch 6/10\n",
      "160000/160000 [==============================] - 4s 23us/step - loss: 0.6865 - auc_2: 0.8879 - auc_3: 0.8948 - val_loss: 0.6875 - val_auc_2: 0.8821 - val_auc_3: 0.8913\n",
      "\n",
      "Epoch 00006: val_auc_2 improved from 0.88097 to 0.88210, saving model to student_model_FL.h5\n",
      "Epoch 7/10\n",
      "160000/160000 [==============================] - 4s 24us/step - loss: 0.6859 - auc_2: 0.8890 - auc_3: 0.8960 - val_loss: 0.6869 - val_auc_2: 0.8829 - val_auc_3: 0.8923\n",
      "\n",
      "Epoch 00007: val_auc_2 improved from 0.88210 to 0.88288, saving model to student_model_FL.h5\n",
      "Epoch 8/10\n",
      "160000/160000 [==============================] - 4s 24us/step - loss: 0.6855 - auc_2: 0.8890 - auc_3: 0.8970 - val_loss: 0.6879 - val_auc_2: 0.8831 - val_auc_3: 0.8930\n",
      "\n",
      "Epoch 00008: val_auc_2 improved from 0.88288 to 0.88307, saving model to student_model_FL.h5\n",
      "Epoch 9/10\n",
      "160000/160000 [==============================] - 4s 25us/step - loss: 0.6851 - auc_2: 0.8899 - auc_3: 0.8977 - val_loss: 0.6878 - val_auc_2: 0.8848 - val_auc_3: 0.8938\n",
      "\n",
      "Epoch 00009: val_auc_2 improved from 0.88307 to 0.88484, saving model to student_model_FL.h5\n",
      "Epoch 10/10\n",
      "160000/160000 [==============================] - 4s 24us/step - loss: 0.6847 - auc_2: 0.8899 - auc_3: 0.8980 - val_loss: 0.6857 - val_auc_2: 0.8857 - val_auc_3: 0.8948\n",
      "\n",
      "Epoch 00010: val_auc_2 improved from 0.88484 to 0.88568, saving model to student_model_FL.h5\n"
     ]
    }
   ],
   "source": [
    "# model = create_model((train.shape[1],), 2)\n",
    "model = create_model((NUM_FEATURES,1), 2)\n",
    "model.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2, auc_3])\n",
    "\n",
    "checkpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_auc_2\", \n",
    "                      mode=\"max\", \n",
    "                      patience=9)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "\n",
    "history = model.fit(x_train,y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec61ffa8dc03af2c7a8c88e59bee2edb8b34d740"
   },
   "source": [
    "## Experment 4\n",
    "Tuning hyper parameter \"Temperature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "3283fdd87f84de160fbcfd8002aed16a7cd866ef"
   },
   "outputs": [],
   "source": [
    "from scipy.special import logit\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "\n",
    "TEMPERATURE = 2\n",
    "\n",
    "y_knowledge_logit = logit(y_knowledge)\n",
    "y_temperature = sigmoid(y_knowledge_logit/TEMPERATURE)\n",
    "\n",
    "# del x_train, x_valid; gc.collect()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid  = train_test_split(train, y, y_temperature,\n",
    "                                                                                             stratify=y, test_size=0.2, random_state=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "ac8842dc0d6dacb8c062cf7cfa3edcd4799e4a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.06985795])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.vstack((y_train, y_knowledge_train)).T\n",
    "y_valid = np.vstack((y_valid, y_knowledge_valid)).T\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "3ecf279fbb4693b6e6e2d55d1d048e520c858f22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/10\n",
      "160000/160000 [==============================] - 4s 27us/step - loss: 0.9028 - auc_2: 0.8037 - auc_3: 0.8252 - val_loss: 0.8877 - val_auc_2: 0.8641 - val_auc_3: 0.8742\n",
      "\n",
      "Epoch 00001: val_auc_2 improved from -inf to 0.86412, saving model to student_model_FL.h5\n",
      "Epoch 2/10\n",
      "160000/160000 [==============================] - 4s 24us/step - loss: 0.8841 - auc_2: 0.8751 - auc_3: 0.8846 - val_loss: 0.8844 - val_auc_2: 0.8760 - val_auc_3: 0.8860\n",
      "\n",
      "Epoch 00002: val_auc_2 improved from 0.86412 to 0.87600, saving model to student_model_FL.h5\n",
      "Epoch 3/10\n",
      "160000/160000 [==============================] - 4s 24us/step - loss: 0.8819 - auc_2: 0.8834 - auc_3: 0.8923 - val_loss: 0.8827 - val_auc_2: 0.8811 - val_auc_3: 0.8900\n",
      "\n",
      "Epoch 00003: val_auc_2 improved from 0.87600 to 0.88106, saving model to student_model_FL.h5\n",
      "Epoch 4/10\n",
      "160000/160000 [==============================] - 4s 23us/step - loss: 0.8810 - auc_2: 0.8872 - auc_3: 0.8953 - val_loss: 0.8820 - val_auc_2: 0.8829 - val_auc_3: 0.8920\n",
      "\n",
      "Epoch 00004: val_auc_2 improved from 0.88106 to 0.88293, saving model to student_model_FL.h5\n",
      "Epoch 5/10\n",
      "160000/160000 [==============================] - 4s 23us/step - loss: 0.8804 - auc_2: 0.8882 - auc_3: 0.8966 - val_loss: 0.8816 - val_auc_2: 0.8826 - val_auc_3: 0.8938\n",
      "\n",
      "Epoch 00005: val_auc_2 did not improve from 0.88293\n",
      "Epoch 6/10\n",
      "160000/160000 [==============================] - 4s 23us/step - loss: 0.8801 - auc_2: 0.8895 - auc_3: 0.8980 - val_loss: 0.8814 - val_auc_2: 0.8844 - val_auc_3: 0.8948\n",
      "\n",
      "Epoch 00006: val_auc_2 improved from 0.88293 to 0.88440, saving model to student_model_FL.h5\n",
      "Epoch 7/10\n",
      "160000/160000 [==============================] - 4s 24us/step - loss: 0.8797 - auc_2: 0.8900 - auc_3: 0.8993 - val_loss: 0.8810 - val_auc_2: 0.8853 - val_auc_3: 0.8961\n",
      "\n",
      "Epoch 00007: val_auc_2 improved from 0.88440 to 0.88525, saving model to student_model_FL.h5\n",
      "Epoch 8/10\n",
      "160000/160000 [==============================] - 4s 24us/step - loss: 0.8797 - auc_2: 0.8906 - auc_3: 0.8998 - val_loss: 0.8810 - val_auc_2: 0.8851 - val_auc_3: 0.8964\n",
      "\n",
      "Epoch 00008: val_auc_2 did not improve from 0.88525\n",
      "Epoch 9/10\n",
      "160000/160000 [==============================] - 4s 23us/step - loss: 0.8796 - auc_2: 0.8920 - auc_3: 0.9006 - val_loss: 0.8806 - val_auc_2: 0.8856 - val_auc_3: 0.8968\n",
      "\n",
      "Epoch 00009: val_auc_2 improved from 0.88525 to 0.88563, saving model to student_model_FL.h5\n",
      "Epoch 10/10\n",
      "160000/160000 [==============================] - 4s 22us/step - loss: 0.8795 - auc_2: 0.8925 - auc_3: 0.9010 - val_loss: 0.8809 - val_auc_2: 0.8864 - val_auc_3: 0.8976\n",
      "\n",
      "Epoch 00010: val_auc_2 improved from 0.88563 to 0.88638, saving model to student_model_FL.h5\n"
     ]
    }
   ],
   "source": [
    "# model = create_model((train.shape[1],), 2)\n",
    "model = create_model((NUM_FEATURES,1), 2)\n",
    "model.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2,auc_3])\n",
    "\n",
    "checkpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_auc_2\", \n",
    "                      mode=\"max\", \n",
    "                      patience=9)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "\n",
    "history = model.fit(x_train,y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size = 1024,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "3ad7d4d62ed9f722334fbff7ccb6a513d15b98d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000,)\n",
      "(200000,)\n",
      "\n",
      "===================FOLD= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159999 samples, validate on 40001 samples\n",
      "Epoch 1/100\n",
      "159999/159999 [==============================] - 4s 28us/step - loss: 0.9026 - auc_2: 0.7925 - auc_3: 0.8315 - val_loss: 0.8863 - val_auc_2: 0.8649 - val_auc_3: 0.8783\n",
      "\n",
      "Epoch 00001: val_auc_2 improved from -inf to 0.86489, saving model to student_model_FL.h5\n",
      "Epoch 2/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8840 - auc_2: 0.8731 - auc_3: 0.8850 - val_loss: 0.8827 - val_auc_2: 0.8775 - val_auc_3: 0.8913\n",
      "\n",
      "Epoch 00002: val_auc_2 improved from 0.86489 to 0.87753, saving model to student_model_FL.h5\n",
      "Epoch 3/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8818 - auc_2: 0.8816 - auc_3: 0.8919 - val_loss: 0.8817 - val_auc_2: 0.8836 - val_auc_3: 0.8950\n",
      "\n",
      "Epoch 00003: val_auc_2 improved from 0.87753 to 0.88363, saving model to student_model_FL.h5\n",
      "Epoch 4/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8811 - auc_2: 0.8848 - auc_3: 0.8942 - val_loss: 0.8810 - val_auc_2: 0.8859 - val_auc_3: 0.8971\n",
      "\n",
      "Epoch 00004: val_auc_2 improved from 0.88363 to 0.88587, saving model to student_model_FL.h5\n",
      "Epoch 5/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8806 - auc_2: 0.8870 - auc_3: 0.8960 - val_loss: 0.8807 - val_auc_2: 0.8873 - val_auc_3: 0.8988\n",
      "\n",
      "Epoch 00005: val_auc_2 improved from 0.88587 to 0.88728, saving model to student_model_FL.h5\n",
      "Epoch 6/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8804 - auc_2: 0.8872 - auc_3: 0.8969 - val_loss: 0.8806 - val_auc_2: 0.8889 - val_auc_3: 0.8997\n",
      "\n",
      "Epoch 00006: val_auc_2 improved from 0.88728 to 0.88892, saving model to student_model_FL.h5\n",
      "Epoch 7/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8805 - auc_2: 0.8883 - auc_3: 0.8979 - val_loss: 0.8805 - val_auc_2: 0.8886 - val_auc_3: 0.9006\n",
      "\n",
      "Epoch 00007: val_auc_2 did not improve from 0.88892\n",
      "Epoch 8/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8800 - auc_2: 0.8888 - auc_3: 0.8986 - val_loss: 0.8804 - val_auc_2: 0.8888 - val_auc_3: 0.9010\n",
      "\n",
      "Epoch 00008: val_auc_2 did not improve from 0.88892\n",
      "Epoch 9/100\n",
      "159999/159999 [==============================] - 4s 25us/step - loss: 0.8798 - auc_2: 0.8896 - auc_3: 0.8992 - val_loss: 0.8798 - val_auc_2: 0.8889 - val_auc_3: 0.9019\n",
      "\n",
      "Epoch 00009: val_auc_2 did not improve from 0.88892\n",
      "Epoch 10/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8798 - auc_2: 0.8894 - auc_3: 0.8995 - val_loss: 0.8814 - val_auc_2: 0.8895 - val_auc_3: 0.9023\n",
      "\n",
      "Epoch 00010: val_auc_2 improved from 0.88892 to 0.88955, saving model to student_model_FL.h5\n",
      "Epoch 11/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8796 - auc_2: 0.8910 - auc_3: 0.8999 - val_loss: 0.8797 - val_auc_2: 0.8909 - val_auc_3: 0.9027\n",
      "\n",
      "Epoch 00011: val_auc_2 improved from 0.88955 to 0.89089, saving model to student_model_FL.h5\n",
      "Epoch 12/100\n",
      "159999/159999 [==============================] - 4s 25us/step - loss: 0.8796 - auc_2: 0.8911 - auc_3: 0.9001 - val_loss: 0.8802 - val_auc_2: 0.8899 - val_auc_3: 0.9026\n",
      "\n",
      "Epoch 00012: val_auc_2 did not improve from 0.89089\n",
      "Epoch 13/100\n",
      "159999/159999 [==============================] - 4s 25us/step - loss: 0.8797 - auc_2: 0.8913 - auc_3: 0.9006 - val_loss: 0.8799 - val_auc_2: 0.8916 - val_auc_3: 0.9029\n",
      "\n",
      "Epoch 00013: val_auc_2 improved from 0.89089 to 0.89162, saving model to student_model_FL.h5\n",
      "Epoch 14/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8796 - auc_2: 0.8920 - auc_3: 0.9005 - val_loss: 0.8796 - val_auc_2: 0.8924 - val_auc_3: 0.9032\n",
      "\n",
      "Epoch 00014: val_auc_2 improved from 0.89162 to 0.89238, saving model to student_model_FL.h5\n",
      "Epoch 15/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8793 - auc_2: 0.8925 - auc_3: 0.9009 - val_loss: 0.8817 - val_auc_2: 0.8924 - val_auc_3: 0.9030\n",
      "\n",
      "Epoch 00015: val_auc_2 did not improve from 0.89238\n",
      "Epoch 16/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8795 - auc_2: 0.8930 - auc_3: 0.9010 - val_loss: 0.8794 - val_auc_2: 0.8940 - val_auc_3: 0.9036\n",
      "\n",
      "Epoch 00016: val_auc_2 improved from 0.89238 to 0.89396, saving model to student_model_FL.h5\n",
      "Epoch 17/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8796 - auc_2: 0.8933 - auc_3: 0.9009 - val_loss: 0.8797 - val_auc_2: 0.8920 - val_auc_3: 0.9037\n",
      "\n",
      "Epoch 00017: val_auc_2 did not improve from 0.89396\n",
      "Epoch 18/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8797 - auc_2: 0.8942 - auc_3: 0.9012 - val_loss: 0.8794 - val_auc_2: 0.8936 - val_auc_3: 0.9040\n",
      "\n",
      "Epoch 00018: val_auc_2 did not improve from 0.89396\n",
      "Epoch 19/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8792 - auc_2: 0.8938 - auc_3: 0.9012 - val_loss: 0.8793 - val_auc_2: 0.8929 - val_auc_3: 0.9041\n",
      "\n",
      "Epoch 00019: val_auc_2 did not improve from 0.89396\n",
      "Epoch 20/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8791 - auc_2: 0.8933 - auc_3: 0.9013 - val_loss: 0.8793 - val_auc_2: 0.8934 - val_auc_3: 0.9041\n",
      "\n",
      "Epoch 00020: val_auc_2 did not improve from 0.89396\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 21/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8789 - auc_2: 0.8975 - auc_3: 0.9017 - val_loss: 0.8792 - val_auc_2: 0.8965 - val_auc_3: 0.9043\n",
      "\n",
      "Epoch 00021: val_auc_2 improved from 0.89396 to 0.89653, saving model to student_model_FL.h5\n",
      "Epoch 22/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8789 - auc_2: 0.8982 - auc_3: 0.9017 - val_loss: 0.8796 - val_auc_2: 0.8969 - val_auc_3: 0.9043\n",
      "\n",
      "Epoch 00022: val_auc_2 improved from 0.89653 to 0.89689, saving model to student_model_FL.h5\n",
      "Epoch 23/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8788 - auc_2: 0.8983 - auc_3: 0.9017 - val_loss: 0.8792 - val_auc_2: 0.8974 - val_auc_3: 0.9045\n",
      "\n",
      "Epoch 00023: val_auc_2 improved from 0.89689 to 0.89741, saving model to student_model_FL.h5\n",
      "Epoch 24/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8789 - auc_2: 0.8979 - auc_3: 0.9019 - val_loss: 0.8795 - val_auc_2: 0.8967 - val_auc_3: 0.9048\n",
      "\n",
      "Epoch 00024: val_auc_2 did not improve from 0.89741\n",
      "Epoch 25/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8789 - auc_2: 0.8979 - auc_3: 0.9018 - val_loss: 0.8792 - val_auc_2: 0.8966 - val_auc_3: 0.9046\n",
      "\n",
      "Epoch 00025: val_auc_2 did not improve from 0.89741\n",
      "Epoch 26/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8788 - auc_2: 0.8979 - auc_3: 0.9018 - val_loss: 0.8791 - val_auc_2: 0.8955 - val_auc_3: 0.9044\n",
      "\n",
      "Epoch 00026: val_auc_2 did not improve from 0.89741\n",
      "Epoch 27/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8788 - auc_2: 0.8977 - auc_3: 0.9019 - val_loss: 0.8791 - val_auc_2: 0.8950 - val_auc_3: 0.9046\n",
      "\n",
      "Epoch 00027: val_auc_2 did not improve from 0.89741\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 28/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8787 - auc_2: 0.9004 - auc_3: 0.9022 - val_loss: 0.8790 - val_auc_2: 0.8986 - val_auc_3: 0.9047\n",
      "\n",
      "Epoch 00028: val_auc_2 improved from 0.89741 to 0.89855, saving model to student_model_FL.h5\n",
      "Epoch 29/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8787 - auc_2: 0.9006 - auc_3: 0.9022 - val_loss: 0.8790 - val_auc_2: 0.8986 - val_auc_3: 0.9048\n",
      "\n",
      "Epoch 00029: val_auc_2 did not improve from 0.89855\n",
      "Epoch 30/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8787 - auc_2: 0.9003 - auc_3: 0.9021 - val_loss: 0.8790 - val_auc_2: 0.8984 - val_auc_3: 0.9049\n",
      "\n",
      "Epoch 00030: val_auc_2 did not improve from 0.89855\n",
      "Epoch 31/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8787 - auc_2: 0.9006 - auc_3: 0.9023 - val_loss: 0.8790 - val_auc_2: 0.8980 - val_auc_3: 0.9049\n",
      "\n",
      "Epoch 00031: val_auc_2 did not improve from 0.89855\n",
      "Epoch 32/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8787 - auc_2: 0.9006 - auc_3: 0.9023 - val_loss: 0.8791 - val_auc_2: 0.8983 - val_auc_3: 0.9049\n",
      "\n",
      "Epoch 00032: val_auc_2 did not improve from 0.89855\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 33/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8786 - auc_2: 0.9020 - auc_3: 0.9024 - val_loss: 0.8790 - val_auc_2: 0.8993 - val_auc_3: 0.9049\n",
      "\n",
      "Epoch 00033: val_auc_2 improved from 0.89855 to 0.89928, saving model to student_model_FL.h5\n",
      "Epoch 34/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9023 - auc_3: 0.9025 - val_loss: 0.8790 - val_auc_2: 0.8990 - val_auc_3: 0.9050\n",
      "\n",
      "Epoch 00034: val_auc_2 did not improve from 0.89928\n",
      "Epoch 35/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9020 - auc_3: 0.9022 - val_loss: 0.8789 - val_auc_2: 0.8988 - val_auc_3: 0.9049\n",
      "\n",
      "Epoch 00035: val_auc_2 did not improve from 0.89928\n",
      "Epoch 36/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8786 - auc_2: 0.9020 - auc_3: 0.9025 - val_loss: 0.8790 - val_auc_2: 0.8988 - val_auc_3: 0.9048\n",
      "\n",
      "Epoch 00036: val_auc_2 did not improve from 0.89928\n",
      "Epoch 37/100\n",
      "159999/159999 [==============================] - 4s 22us/step - loss: 0.8786 - auc_2: 0.9020 - auc_3: 0.9024 - val_loss: 0.8790 - val_auc_2: 0.8994 - val_auc_3: 0.9050\n",
      "\n",
      "Epoch 00037: val_auc_2 improved from 0.89928 to 0.89935, saving model to student_model_FL.h5\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 38/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8786 - auc_2: 0.9029 - auc_3: 0.9024 - val_loss: 0.8789 - val_auc_2: 0.8996 - val_auc_3: 0.9051\n",
      "\n",
      "Epoch 00038: val_auc_2 improved from 0.89935 to 0.89961, saving model to student_model_FL.h5\n",
      "Epoch 39/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9030 - auc_3: 0.9025 - val_loss: 0.8790 - val_auc_2: 0.8997 - val_auc_3: 0.9052\n",
      "\n",
      "Epoch 00039: val_auc_2 improved from 0.89961 to 0.89973, saving model to student_model_FL.h5\n",
      "Epoch 40/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9031 - auc_3: 0.9025 - val_loss: 0.8789 - val_auc_2: 0.8996 - val_auc_3: 0.9051\n",
      "\n",
      "Epoch 00040: val_auc_2 did not improve from 0.89973\n",
      "Epoch 41/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8786 - auc_2: 0.9027 - auc_3: 0.9022 - val_loss: 0.8789 - val_auc_2: 0.8996 - val_auc_3: 0.9050\n",
      "\n",
      "Epoch 00041: val_auc_2 did not improve from 0.89973\n",
      "Epoch 42/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9031 - auc_3: 0.9025 - val_loss: 0.8789 - val_auc_2: 0.8997 - val_auc_3: 0.9052\n",
      "\n",
      "Epoch 00042: val_auc_2 did not improve from 0.89973\n",
      "Epoch 43/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9029 - auc_3: 0.9025 - val_loss: 0.8789 - val_auc_2: 0.8998 - val_auc_3: 0.9052\n",
      "\n",
      "Epoch 00043: val_auc_2 improved from 0.89973 to 0.89984, saving model to student_model_FL.h5\n",
      "Epoch 44/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9028 - auc_3: 0.9022 - val_loss: 0.8789 - val_auc_2: 0.8998 - val_auc_3: 0.9052\n",
      "\n",
      "Epoch 00044: val_auc_2 did not improve from 0.89984\n",
      "Epoch 45/100\n",
      "159999/159999 [==============================] - 4s 22us/step - loss: 0.8786 - auc_2: 0.9028 - auc_3: 0.9023 - val_loss: 0.8789 - val_auc_2: 0.8998 - val_auc_3: 0.9051\n",
      "\n",
      "Epoch 00045: val_auc_2 did not improve from 0.89984\n",
      "Epoch 46/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8786 - auc_2: 0.9030 - auc_3: 0.9024 - val_loss: 0.8789 - val_auc_2: 0.8996 - val_auc_3: 0.9051\n",
      "\n",
      "Epoch 00046: val_auc_2 did not improve from 0.89984\n",
      "Epoch 47/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9032 - auc_3: 0.9026 - val_loss: 0.8789 - val_auc_2: 0.8996 - val_auc_3: 0.9051\n",
      "\n",
      "Epoch 00047: val_auc_2 did not improve from 0.89984\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 48/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8786 - auc_2: 0.9036 - auc_3: 0.9025 - val_loss: 0.8789 - val_auc_2: 0.8997 - val_auc_3: 0.9051\n",
      "\n",
      "Epoch 00048: val_auc_2 did not improve from 0.89984\n",
      "Epoch 49/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9038 - auc_3: 0.9027 - val_loss: 0.8789 - val_auc_2: 0.8998 - val_auc_3: 0.9052\n",
      "\n",
      "Epoch 00049: val_auc_2 did not improve from 0.89984\n",
      "Epoch 50/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9035 - auc_3: 0.9024 - val_loss: 0.8789 - val_auc_2: 0.8998 - val_auc_3: 0.9052\n",
      "\n",
      "Epoch 00050: val_auc_2 did not improve from 0.89984\n",
      "Epoch 51/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9038 - auc_3: 0.9027 - val_loss: 0.8789 - val_auc_2: 0.8998 - val_auc_3: 0.9052\n",
      "\n",
      "Epoch 00051: val_auc_2 did not improve from 0.89984\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 52/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9040 - auc_3: 0.9025 - val_loss: 0.8789 - val_auc_2: 0.8998 - val_auc_3: 0.9052\n",
      "\n",
      "Epoch 00052: val_auc_2 did not improve from 0.89984\n",
      "40001/40001 [==============================] - 0s 10us/step\n",
      "200000/200000 [==============================] - 2s 12us/step\n",
      "40001/40001 [==============================] - 0s 6us/step\n",
      "\n",
      "===================FOLD= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159999 samples, validate on 40001 samples\n",
      "Epoch 1/100\n",
      "159999/159999 [==============================] - 5s 29us/step - loss: 0.9055 - auc_2: 0.7875 - auc_3: 0.8209 - val_loss: 0.8870 - val_auc_2: 0.8618 - val_auc_3: 0.8733\n",
      "\n",
      "Epoch 00001: val_auc_2 improved from -inf to 0.86183, saving model to student_model_FL.h5\n",
      "Epoch 2/100\n",
      "159999/159999 [==============================] - 4s 27us/step - loss: 0.8847 - auc_2: 0.8733 - auc_3: 0.8836 - val_loss: 0.8830 - val_auc_2: 0.8759 - val_auc_3: 0.8868\n",
      "\n",
      "Epoch 00002: val_auc_2 improved from 0.86183 to 0.87593, saving model to student_model_FL.h5\n",
      "Epoch 3/100\n",
      "159999/159999 [==============================] - 4s 25us/step - loss: 0.8818 - auc_2: 0.8837 - auc_3: 0.8929 - val_loss: 0.8811 - val_auc_2: 0.8833 - val_auc_3: 0.8934\n",
      "\n",
      "Epoch 00003: val_auc_2 improved from 0.87593 to 0.88334, saving model to student_model_FL.h5\n",
      "Epoch 4/100\n",
      "159999/159999 [==============================] - 4s 28us/step - loss: 0.8806 - auc_2: 0.8881 - auc_3: 0.8968 - val_loss: 0.8806 - val_auc_2: 0.8860 - val_auc_3: 0.8962\n",
      "\n",
      "Epoch 00004: val_auc_2 improved from 0.88334 to 0.88599, saving model to student_model_FL.h5\n",
      "Epoch 5/100\n",
      "159999/159999 [==============================] - 5s 30us/step - loss: 0.8801 - auc_2: 0.8902 - auc_3: 0.8983 - val_loss: 0.8799 - val_auc_2: 0.8865 - val_auc_3: 0.8978\n",
      "\n",
      "Epoch 00005: val_auc_2 improved from 0.88599 to 0.88655, saving model to student_model_FL.h5\n",
      "Epoch 6/100\n",
      "159999/159999 [==============================] - 4s 27us/step - loss: 0.8798 - auc_2: 0.8906 - auc_3: 0.8996 - val_loss: 0.8797 - val_auc_2: 0.8882 - val_auc_3: 0.8985\n",
      "\n",
      "Epoch 00006: val_auc_2 improved from 0.88655 to 0.88824, saving model to student_model_FL.h5\n",
      "Epoch 7/100\n",
      "159999/159999 [==============================] - 5s 29us/step - loss: 0.8796 - auc_2: 0.8914 - auc_3: 0.9003 - val_loss: 0.8795 - val_auc_2: 0.8878 - val_auc_3: 0.8992\n",
      "\n",
      "Epoch 00007: val_auc_2 did not improve from 0.88824\n",
      "Epoch 8/100\n",
      "159999/159999 [==============================] - 4s 28us/step - loss: 0.8795 - auc_2: 0.8919 - auc_3: 0.9007 - val_loss: 0.8794 - val_auc_2: 0.8887 - val_auc_3: 0.8993\n",
      "\n",
      "Epoch 00008: val_auc_2 improved from 0.88824 to 0.88868, saving model to student_model_FL.h5\n",
      "Epoch 9/100\n",
      "159999/159999 [==============================] - 4s 25us/step - loss: 0.8797 - auc_2: 0.8917 - auc_3: 0.9012 - val_loss: 0.8794 - val_auc_2: 0.8897 - val_auc_3: 0.8998\n",
      "\n",
      "Epoch 00009: val_auc_2 improved from 0.88868 to 0.88973, saving model to student_model_FL.h5\n",
      "Epoch 10/100\n",
      "159999/159999 [==============================] - 4s 27us/step - loss: 0.8795 - auc_2: 0.8934 - auc_3: 0.9013 - val_loss: 0.8793 - val_auc_2: 0.8907 - val_auc_3: 0.9002\n",
      "\n",
      "Epoch 00010: val_auc_2 improved from 0.88973 to 0.89074, saving model to student_model_FL.h5\n",
      "Epoch 11/100\n",
      "159999/159999 [==============================] - 5s 28us/step - loss: 0.8795 - auc_2: 0.8928 - auc_3: 0.9013 - val_loss: 0.8793 - val_auc_2: 0.8892 - val_auc_3: 0.9002\n",
      "\n",
      "Epoch 00011: val_auc_2 did not improve from 0.89074\n",
      "Epoch 12/100\n",
      "159999/159999 [==============================] - 4s 27us/step - loss: 0.8794 - auc_2: 0.8935 - auc_3: 0.9016 - val_loss: 0.8795 - val_auc_2: 0.8903 - val_auc_3: 0.9003\n",
      "\n",
      "Epoch 00012: val_auc_2 did not improve from 0.89074\n",
      "Epoch 13/100\n",
      "159999/159999 [==============================] - 4s 26us/step - loss: 0.8794 - auc_2: 0.8939 - auc_3: 0.9017 - val_loss: 0.8792 - val_auc_2: 0.8901 - val_auc_3: 0.9006\n",
      "\n",
      "Epoch 00013: val_auc_2 did not improve from 0.89074\n",
      "Epoch 14/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8793 - auc_2: 0.8939 - auc_3: 0.9019 - val_loss: 0.8793 - val_auc_2: 0.8911 - val_auc_3: 0.9006\n",
      "\n",
      "Epoch 00014: val_auc_2 improved from 0.89074 to 0.89111, saving model to student_model_FL.h5\n",
      "Epoch 15/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8793 - auc_2: 0.8942 - auc_3: 0.9021 - val_loss: 0.8793 - val_auc_2: 0.8893 - val_auc_3: 0.9008\n",
      "\n",
      "Epoch 00015: val_auc_2 did not improve from 0.89111\n",
      "Epoch 16/100\n",
      "159999/159999 [==============================] - 4s 25us/step - loss: 0.8794 - auc_2: 0.8949 - auc_3: 0.9025 - val_loss: 0.8793 - val_auc_2: 0.8912 - val_auc_3: 0.9010\n",
      "\n",
      "Epoch 00016: val_auc_2 improved from 0.89111 to 0.89118, saving model to student_model_FL.h5\n",
      "Epoch 17/100\n",
      "159999/159999 [==============================] - 4s 25us/step - loss: 0.8792 - auc_2: 0.8956 - auc_3: 0.9022 - val_loss: 0.8790 - val_auc_2: 0.8916 - val_auc_3: 0.9009\n",
      "\n",
      "Epoch 00017: val_auc_2 improved from 0.89118 to 0.89164, saving model to student_model_FL.h5\n",
      "Epoch 18/100\n",
      "159999/159999 [==============================] - 4s 25us/step - loss: 0.8792 - auc_2: 0.8958 - auc_3: 0.9025 - val_loss: 0.8795 - val_auc_2: 0.8893 - val_auc_3: 0.9010\n",
      "\n",
      "Epoch 00018: val_auc_2 did not improve from 0.89164\n",
      "Epoch 19/100\n",
      "159999/159999 [==============================] - 4s 26us/step - loss: 0.8791 - auc_2: 0.8960 - auc_3: 0.9026 - val_loss: 0.8800 - val_auc_2: 0.8904 - val_auc_3: 0.9011\n",
      "\n",
      "Epoch 00019: val_auc_2 did not improve from 0.89164\n",
      "Epoch 20/100\n",
      "159999/159999 [==============================] - 4s 27us/step - loss: 0.8791 - auc_2: 0.8959 - auc_3: 0.9027 - val_loss: 0.8792 - val_auc_2: 0.8917 - val_auc_3: 0.9012\n",
      "\n",
      "Epoch 00020: val_auc_2 improved from 0.89164 to 0.89167, saving model to student_model_FL.h5\n",
      "Epoch 21/100\n",
      "159999/159999 [==============================] - 4s 28us/step - loss: 0.8790 - auc_2: 0.8959 - auc_3: 0.9028 - val_loss: 0.8794 - val_auc_2: 0.8900 - val_auc_3: 0.9013\n",
      "\n",
      "Epoch 00021: val_auc_2 did not improve from 0.89167\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 22/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8788 - auc_2: 0.9001 - auc_3: 0.9032 - val_loss: 0.8788 - val_auc_2: 0.8945 - val_auc_3: 0.9015\n",
      "\n",
      "Epoch 00022: val_auc_2 improved from 0.89167 to 0.89452, saving model to student_model_FL.h5\n",
      "Epoch 23/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8788 - auc_2: 0.9001 - auc_3: 0.9032 - val_loss: 0.8788 - val_auc_2: 0.8942 - val_auc_3: 0.9017\n",
      "\n",
      "Epoch 00023: val_auc_2 did not improve from 0.89452\n",
      "Epoch 24/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8787 - auc_2: 0.8998 - auc_3: 0.9029 - val_loss: 0.8789 - val_auc_2: 0.8954 - val_auc_3: 0.9017\n",
      "\n",
      "Epoch 00024: val_auc_2 improved from 0.89452 to 0.89540, saving model to student_model_FL.h5\n",
      "Epoch 25/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8787 - auc_2: 0.9001 - auc_3: 0.9030 - val_loss: 0.8788 - val_auc_2: 0.8940 - val_auc_3: 0.9015\n",
      "\n",
      "Epoch 00025: val_auc_2 did not improve from 0.89540\n",
      "Epoch 26/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8787 - auc_2: 0.8999 - auc_3: 0.9035 - val_loss: 0.8789 - val_auc_2: 0.8949 - val_auc_3: 0.9018\n",
      "\n",
      "Epoch 00026: val_auc_2 did not improve from 0.89540\n",
      "Epoch 27/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8788 - auc_2: 0.9000 - auc_3: 0.9032 - val_loss: 0.8788 - val_auc_2: 0.8945 - val_auc_3: 0.9020\n",
      "\n",
      "Epoch 00027: val_auc_2 did not improve from 0.89540\n",
      "Epoch 28/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8787 - auc_2: 0.9004 - auc_3: 0.9036 - val_loss: 0.8788 - val_auc_2: 0.8937 - val_auc_3: 0.9019\n",
      "\n",
      "Epoch 00028: val_auc_2 did not improve from 0.89540\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 29/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9021 - auc_3: 0.9035 - val_loss: 0.8787 - val_auc_2: 0.8962 - val_auc_3: 0.9020\n",
      "\n",
      "Epoch 00029: val_auc_2 improved from 0.89540 to 0.89619, saving model to student_model_FL.h5\n",
      "Epoch 30/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9024 - auc_3: 0.9034 - val_loss: 0.8787 - val_auc_2: 0.8966 - val_auc_3: 0.9021\n",
      "\n",
      "Epoch 00030: val_auc_2 improved from 0.89619 to 0.89663, saving model to student_model_FL.h5\n",
      "Epoch 31/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9024 - auc_3: 0.9036 - val_loss: 0.8787 - val_auc_2: 0.8953 - val_auc_3: 0.9019\n",
      "\n",
      "Epoch 00031: val_auc_2 did not improve from 0.89663\n",
      "Epoch 32/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9025 - auc_3: 0.9038 - val_loss: 0.8788 - val_auc_2: 0.8959 - val_auc_3: 0.9020\n",
      "\n",
      "Epoch 00032: val_auc_2 did not improve from 0.89663\n",
      "Epoch 33/100\n",
      "159999/159999 [==============================] - 4s 25us/step - loss: 0.8786 - auc_2: 0.9024 - auc_3: 0.9036 - val_loss: 0.8788 - val_auc_2: 0.8952 - val_auc_3: 0.9019\n",
      "\n",
      "Epoch 00033: val_auc_2 did not improve from 0.89663\n",
      "Epoch 34/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9022 - auc_3: 0.9035 - val_loss: 0.8787 - val_auc_2: 0.8950 - val_auc_3: 0.9020\n",
      "\n",
      "Epoch 00034: val_auc_2 did not improve from 0.89663\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 35/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9036 - auc_3: 0.9036 - val_loss: 0.8786 - val_auc_2: 0.8964 - val_auc_3: 0.9022\n",
      "\n",
      "Epoch 00035: val_auc_2 did not improve from 0.89663\n",
      "Epoch 36/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8786 - auc_2: 0.9038 - auc_3: 0.9038 - val_loss: 0.8786 - val_auc_2: 0.8964 - val_auc_3: 0.9022\n",
      "\n",
      "Epoch 00036: val_auc_2 did not improve from 0.89663\n",
      "Epoch 37/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9039 - auc_3: 0.9038 - val_loss: 0.8786 - val_auc_2: 0.8964 - val_auc_3: 0.9021\n",
      "\n",
      "Epoch 00037: val_auc_2 did not improve from 0.89663\n",
      "Epoch 38/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9042 - auc_3: 0.9039 - val_loss: 0.8786 - val_auc_2: 0.8965 - val_auc_3: 0.9022\n",
      "\n",
      "Epoch 00038: val_auc_2 did not improve from 0.89663\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 39/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9046 - auc_3: 0.9037 - val_loss: 0.8786 - val_auc_2: 0.8968 - val_auc_3: 0.9022\n",
      "\n",
      "Epoch 00039: val_auc_2 improved from 0.89663 to 0.89678, saving model to student_model_FL.h5\n",
      "Epoch 40/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9050 - auc_3: 0.9040 - val_loss: 0.8786 - val_auc_2: 0.8966 - val_auc_3: 0.9022\n",
      "\n",
      "Epoch 00040: val_auc_2 did not improve from 0.89678\n",
      "Epoch 41/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9047 - auc_3: 0.9038 - val_loss: 0.8786 - val_auc_2: 0.8965 - val_auc_3: 0.9022\n",
      "\n",
      "Epoch 00041: val_auc_2 did not improve from 0.89678\n",
      "Epoch 42/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9047 - auc_3: 0.9038 - val_loss: 0.8786 - val_auc_2: 0.8967 - val_auc_3: 0.9022\n",
      "\n",
      "Epoch 00042: val_auc_2 did not improve from 0.89678\n",
      "Epoch 43/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9046 - auc_3: 0.9037 - val_loss: 0.8786 - val_auc_2: 0.8966 - val_auc_3: 0.9023\n",
      "\n",
      "Epoch 00043: val_auc_2 did not improve from 0.89678\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 44/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8785 - auc_2: 0.9053 - auc_3: 0.9040 - val_loss: 0.8786 - val_auc_2: 0.8967 - val_auc_3: 0.9023\n",
      "\n",
      "Epoch 00044: val_auc_2 did not improve from 0.89678\n",
      "Epoch 45/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8785 - auc_2: 0.9052 - auc_3: 0.9039 - val_loss: 0.8786 - val_auc_2: 0.8967 - val_auc_3: 0.9023\n",
      "\n",
      "Epoch 00045: val_auc_2 did not improve from 0.89678\n",
      "Epoch 46/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9055 - auc_3: 0.9040 - val_loss: 0.8786 - val_auc_2: 0.8968 - val_auc_3: 0.9023\n",
      "\n",
      "Epoch 00046: val_auc_2 improved from 0.89678 to 0.89683, saving model to student_model_FL.h5\n",
      "Epoch 47/100\n",
      "159999/159999 [==============================] - 4s 24us/step - loss: 0.8785 - auc_2: 0.9053 - auc_3: 0.9040 - val_loss: 0.8786 - val_auc_2: 0.8968 - val_auc_3: 0.9023\n",
      "\n",
      "Epoch 00047: val_auc_2 improved from 0.89683 to 0.89683, saving model to student_model_FL.h5\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 48/100\n",
      "159999/159999 [==============================] - 4s 23us/step - loss: 0.8785 - auc_2: 0.9055 - auc_3: 0.9039 - val_loss: 0.8786 - val_auc_2: 0.8968 - val_auc_3: 0.9023\n",
      "\n",
      "Epoch 00048: val_auc_2 improved from 0.89683 to 0.89683, saving model to student_model_FL.h5\n",
      "Epoch 49/100\n",
      " 44032/159999 [=======>......................] - ETA: 2s - loss: 0.8796 - auc_2: 0.9040 - auc_3: 0.9026"
     ]
    }
   ],
   "source": [
    "# run k-fold\n",
    "num_fold = 5\n",
    "folds = list(StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=7).split(train, y))\n",
    "# del x_train, x_valid; gc.collect()\n",
    "\n",
    "y_test_pred_log = np.zeros(len(train))\n",
    "y_train_pred_log = np.zeros(len(train))\n",
    "print(y_test_pred_log.shape)\n",
    "print(y_train_pred_log.shape)\n",
    "score = []\n",
    "\n",
    "for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "    print('\\n===================FOLD=',j)\n",
    "    x_train, x_valid = train[train_idx], train[valid_idx]\n",
    "    y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "    y_knowledge_train, y_knowledge_valid = y_temperature[train_idx], y_temperature[valid_idx]\n",
    "    \n",
    "    y_train = np.vstack((y_train, y_knowledge_train)).T\n",
    "    y_valid = np.vstack((y_valid, y_knowledge_valid)).T\n",
    "    \n",
    "    # model = create_model((train.shape[1],), 2)\n",
    "    model = create_model((NUM_FEATURES,1), 2)\n",
    "    model.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2, auc_3])\n",
    "\n",
    "    checkpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n",
    "                                 save_best_only=True, mode='max', save_weights_only = True)\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n",
    "                                       verbose=1, mode='max', epsilon=0.0001)\n",
    "    early = EarlyStopping(monitor=\"val_auc_2\", \n",
    "                          mode=\"max\", \n",
    "                          patience=9)\n",
    "    callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "    history = model.fit(x_train,y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size = BATCH_SIZE,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=(x_valid, y_valid))\n",
    "    \n",
    "    model.load_weights('student_model_FL.h5')\n",
    "    prediction = model.predict(x_valid,\n",
    "                               batch_size=512,\n",
    "                               verbose=1)\n",
    "    # print(prediction.shape)\n",
    "    # prediction = np.sum(prediction, axis=1)/2\n",
    "    score.append(roc_auc_score(y_valid[:,0], prediction[:,1]))\n",
    "    # score.append(roc_auc_score(y_valid[:,0], prediction))\n",
    "    prediction = model.predict(test,\n",
    "                               batch_size=512,\n",
    "                               verbose=1)\n",
    "    # y_test_pred_log += np.sum(prediction, axis=1)/2\n",
    "    y_test_pred_log += np.squeeze(prediction[:, 1])\n",
    "    \n",
    "    prediction = model.predict(x_valid,\n",
    "                               batch_size=512,\n",
    "                               verbose=1)\n",
    "    # y_train_pred_log += np.sum(prediction, axis=1)/2\n",
    "    y_train_pred_log[valid_idx] += np.squeeze(prediction[:, 1])\n",
    "    \n",
    "    del x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid\n",
    "    gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "5751d2bbb3a9b71a71a7c21f69f7d2173568cbba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF score:  0.9033017259021195\n",
      "average 5 folds score:  0.9033126588081204\n"
     ]
    }
   ],
   "source": [
    "print(\"OOF score: \", roc_auc_score(y, y_train_pred_log/num_fold))\n",
    "print(\"average {} folds score: \".format(num_fold), np.sum(score)/num_fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "a2739c50d9ec082329206a058ab45c5fc9d9beca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.252894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.332368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.290927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.297146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.157675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.252894\n",
       "1  test_1  0.332368\n",
       "2  test_2  0.290927\n",
       "3  test_3  0.297146\n",
       "4  test_4  0.157675"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make submission\n",
    "submit = pd.read_csv('../input/santander-customer-transaction-prediction/sample_submission.csv')\n",
    "submit['ID_code'] = id_code_test\n",
    "submit['target'] = y_test_pred_log/num_fold\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c877067e96b93fe594097e3b6851888fc6826d00"
   },
   "source": [
    "# Please upvote if you find this kernel interesting ^_^"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
