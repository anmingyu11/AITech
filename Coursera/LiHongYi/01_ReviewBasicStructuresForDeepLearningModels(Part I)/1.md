### FullyConnectedLayer

为什么要写上
$$
W_{ij}
$$
因为如果不这么写后面的
$$
W
$$
就要写成
$$
W^T
$$


## Primidal RNN

Sequence无法并行，

金字塔可以减少计算量，不然rnn太大无法收敛



## LSTM

两条路径一条快的一条慢的

慢的那条可以记忆比较过去的information

![[公式]](https://www.zhihu.com/equation?tex=%5Codot) 是Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。 ![[公式]](https://www.zhihu.com/equation?tex=%5Coplus) 则代表进行矩阵加法。



## GRU

GRU的运算量比LSTM少

减小参数量不一定要通过拿掉一个gate来完成，可以通过减少参数的维度，但是为什么这么做呢

forget gate

把新的东西加进去，就要把旧的东西忘记



对于时间序列的数据

lag feature比较好



## LSTM & RNN

LSTM 解决梯度消失

LSTM 比较容一给一个好的learning rate, rnn 不好给，一般给一个较小的



## An Empirical Exploration of Recurrent Network Architectures

Corpus

Arith : 两排数字，数字加或减，预测正确结果

xml: 给xml的文件，predict 下一个字符 

PTB: 给一串文字，predict 下一个word 是什么



Hyperbolic tangent function

tanh



LSTM - f 拿掉 forget gate

LSTM -i  拿掉 input gate



大的 forget gate bias 意味着偏好于 记住之前的输入